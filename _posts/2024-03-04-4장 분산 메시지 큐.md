---
layout: post
title:  "4장. 분산 메시지 큐"
date:   2024-03-04 00:00:00 +0900
categories: study books system-design-interview-vol2
announced_on: 2024년 3월 4일
author: 한재민
---
흔히 메시지 큐는 RabbitMQ와 Kafka 등등으로 알려져 있지만, 엄밀하게 보면 다름.

[메시지 대기열이란 무엇인가요?](https://aws.amazon.com/ko/message-queue/)

가장 큰 차이는 이벤트 스트리밍의 여부. 그러나 메시지 큐와 이벤트 스트리밍의 기능이 서로 비슷해지면서 근래에는 거의 동일한 것으로 취급. 

- 단, 기능적인 특징은 조금씩 다르기 때문에 확인이 필요.

[RabbitMQ와 Kafka - 메시지 대기열 시스템 간의 차이점 - AWS](https://aws.amazon.com/ko/compare/the-difference-between-rabbitmq-and-kafka/)

### 요구사항

- 생산자는 메시지 큐에 메시지를 보낼 수 있어야 함.
- 소비자는 메시지 큐를 통해 메시지를 수신할 수 있어야 함.
- 메시지 ‘반복적 수신’ or ‘단한번 수신’ 설정
- 오래된 이력 데이터 삭제 가능
- 메시지는 생산된 순서대로 전달
- 메시지 전달 방식은 ‘최소 한 번’, ‘최대 한 번’, ‘정확히 한 번’ 설정 필요

### 비기능 요구사항

- 높은 대역폭과 낮은 전송 지연 중 선택 가능
- 규모 확장성
- 지속성(persistency): 디스크에 보관
- 내구성(durability): 여러 노드에 복제

### 개략적 설계안 정리

**메시지 모델 (발행-구독 모델)**

- 토픽을 통해 메시지를 주고 받음
- 일대일 모델 진화 버전 (소비자 그룹 사용)

<aside>
💡 일대일 모델은 전통적인 메시지 큐 방식으로 오직 한 소비자만 가져갈 수 있는 형태

</aside>

**토픽, 파티션, 브로커**

- 메시지는 토픽에 보관
- 토픽에 보관되는 데이터의 양이 서버 한대로 감당하기 힘들 때, 파티션(샤딩) 도입
    - 토픽을 여러 파티션으로 분할 이후 균등 분배
    (파티션을 유지하는 서버는 브로커)
    - 파티션 개수를 늘리면 토픽 용량이 확장 (규모 확장)
- 토픽 파티션은 FIFO(first in, first out)
- 파티션 내의 메시지 위치는 offset
- 동일한 키를 가진 메시지는 동일한 파티션으로! 그렇지 않으면 랜덤 파티션

**소비자 그룹**

- 소비자 그룹은 오프셋을 별도로 관리
- 같은 그룹내 소비자는 메시지를 병렬로 소비 (대역폭 상승)
    - 단, 같은 파티션 안에 메시지를 순서대로 소비할 수 없는 단점.
    - 한 그룹 안에서는 오직 한 소비자만 하나의 파티션만 읽을 수 있도록 제약이 필요함
        - 사실상, 일대일 모델
- 처리 용량은 소비자를 더 추가하면 됨

### 개략적 설계

주요 컴포넌트만.

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled.png)

**브로커**

- 파티션 관리

**저장소**

- 데이터 저장소: 파티션 내 메시지 저장
- 상태 저장소: 소비자 상태 저장
- 메타데이터 저장소: 토픽 설정, 토픽 속성 등 저장

**조정 서비스(coordination service)**

- 서비스 탐색(service discovery): 어떤 브로커가 살아있는지 여부
- 리더 선출(leader election): 브로커 리더 선출
    - 브로커 가운데 하나가 컨트롤러 역할을 하며, 파티션 배치도 수행
    (클러스터에는 반드시 active 상태 컨트롤러가 있어야 함)
    - 일반적으로 아파치 주키퍼나 etcd를 사용함
- 왜 k8s는 etcd를 선택하였을까
    
    etcd의 태생 자체가 Zookeeper의 문제를 새로운 설계를 통해 재창조시킨것임
    
    [etcd versus other key-value stores](https://etcd.io/docs/v3.6/learning/why/#zookeeper)
    
    > We discussed the major features, pros, and cons of Apache ZooKeeper and etcd3. ZooKeeper is written in Java and is widely adopted by Apache Software Foundation projects, while etcd3 is backed by Google (Kubernetes). Even though Apache ZooKeeper is stable and is renowned for being a great distributed coordination system, etcd3 is new and promising.
    > 
    > 
    > 
    > Since etcd is written in Go, good client libraries are not available for Java. In contrast, since ZooKeeper has been around for some time, it has some good client libraries written in other languages, as well. However, whether you should go with ZooKeeper or etcd depends on your requirements and your preferred language.
    > 

### 상세 설계

**주요 전략**

- 디스크 기반 자료 구조 활용
    - 회전 디스크(rotational disk)의 높은 순차 탐색 성능
    - 디스크 캐시 전략(aggressvie disk caching strategy) 활용
- 메시지 복사 비용 최소화
    - 생산자부터 소비자까지 수정 없이 전달
- 일괄 처리(batching) 우선 시스템
    - 소규모 I/O가 많으면 높은 대역폭을 지원하기 어려움
    - 생산자는 메시지 일괄 전송, 소비자는 일괄 수신
    

**데이터 저장소**

지속적으로 유딥되는 메시지를 어떻게 저장할지 트래픽 패턴 분석이 필요

- 읽기와 쓰기가 빈번하게 발생
- 갱신, 삭제 연산 발생 X
- 순차적인 읽기와 쓰기가 대부분

**선택지 1. 데이터베이스**

- 관계형 데이터베이스: 토픽별 테이블
- NoSQL 데이터베이스: 토픽별 컬렉션

<aside>
💡 데이터 저장 요구사항은 맞출 수 있음. 단! 읽기와 쓰기 연산이 대규모로 빈번하게 발생하면 오히려 시스템 병목이 될 수 있음.

</aside>

**선택지2. 쓰기 우선 로그(Write-Ahread Log, WAL)**

추천이지만 사실상 필수!

WAL은 새로운 항목이 추가되기만(append-only)하는 일반 파일로 MySQL의 복구 로그(redo log) 및 아파치 주키퍼에서도 이미 사용하고 있는 전략

[MySQL :: MySQL 8.0: New Lock free, scalable WAL design](https://dev.mysql.com/blog-archive/mysql-8-0-new-lock-free-scalable-wal-design/)

![WAL의 메시지 추가 과정 (레코드가 순차적으로 쓰여짐)](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%201.png)

WAL의 메시지 추가 과정 (레코드가 순차적으로 쓰여짐)

위의 그림을 바탕으로 지속성을 보장해야 하는 메시지가 WAL과 잘 맞물리는 이유는 아래와 같음

- WAL의 접근 패턴은 순차적인 읽기와 쓰기
- 접근 패턴이 순차적일 때 디스크는 높은 성능을 보임
- 디스크 기반 저장장치는 비용이 저렴함

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%202.png)

- **참고! 데이터베이스는 왜 WAL이 필요한 것인가?!**
    
    [Why databases need write-ahead log (WAL) — A deep dive](https://medium.com/@hnasr/what-is-wal-write-ahead-log-a-deep-dive-a2bc4dc91170)
    

오프셋(Offset) vs 세그먼트(Segment)

- 오프셋: 가장 쉬운 방법(로그 파일 줄 번호)이지만 파일의 크기가 무한정 커질 수 없음
- 세그먼트: 일정 크기 이상이 되면 새로운 파일 생성

<aside>
💡 새로운 세그먼트가 만들어지면 이전 세그먼트는 비활성화 상태로 읽기 요청만 처리
(보관 기한이 만료되거나 용량 한계에 도달해버리면 삭제 가능)

</aside>

- 디스크 성능 유의 사항
    
    회전식 디스크는 무작위(random) 접근 패턴일 때 성능이 낮아짐
    
    [RAID (Redundant Array of Independent Disks)](https://velog.io/@moonblue/스토리지-RAID-란)
    
    RAID로 구성된 디스크 드라이브에서 순차적 데이터 접근 패턴을 활용할 때, 
    
    - 수백 MB/sec 수준의 읽기 및 쓰기 성능을 달성할 수 있음
    
    ![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%203.png)
    

**메시지 자료 구조**

전달 과정에서 불필요한 복사가 일어나지 않아야 높은 대역폭이 가능함

![메시지 스키마 (예시)](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%204.png)

메시지 스키마 (예시)

- 메시지키
- 메시지값 (payload)
    - 일반 텍스트 또는 압축된 이진 블록(block)
- 기타 필드
    - 토픽, 파티션, 오프셋
    - 타임스탬프: 메시지가 저장된 시각
    - 크기: 메시지의 크기
    - CRC(Cyclic Redundancy Check)
    
    [CRC(Cyclic Redundancy Check, 순환 중복 검사)](https://advancedtestingservices.tistory.com/214)
    

위의 필드 외에도 tags과 같은 선택적 필드(optional field)를 사용하여 필터링에 사용 할 수도 있음

**생산자 측 작업 흐름**

**최초 방법. 라우팅 계층 도입**

파티션을 관리하는 리더(leader) 브로커에 연결 (브로커가 여러 개일 경우)

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%205.png)

1. 생산자는 메시지를 라우팅 계층에 전달
2. 라우팅 계층에서 메타데이터 저장소에서 사본 분산 계획(replica distribution plan)을 읽어 캐시에 보관하고, 이후에 메시지가 도착하면 파티션-1의 리더 사본에 전달
3. 리더 사본이 우선 메시지를 받고 다른 사본에 전달
4. 충분한 수의 사본이 동기화되면 리더는 데이터를 디스크에 기록(commit) 이후 회신
(소비 가능 상태가 되는 시점)

<aside>
💡 사본 분산 계획

- 각 파티션 사본의 분산 배치 정보를 의미

<img src="/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%206.png" alt="Untitled"/>

</aside>

그러나 최초 방법은 아래와 같은 문제점이 있음

- 네트워크 노드의 추가로 인한 오버헤드로 네트워크 전송 지연 발생
- 일괄처리를 고려하지 않음

**수정된 방법. 버퍼 도입**

생산자 내부에 라우팅 계층 편입 및 버퍼 도입 (producer client library 사용)

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%207.png)

동작 흐름은 비슷하지만 다른점은 아래와 같음

- 네트워크 오버헤드 감소 (전송 지연 감소)
- 일괄 전송을 통한 대역폭 상승 (버퍼 메모리에 저장 이후 한방!)
- 생산자는 메시지를 어느 파티션에 보낼지 결정하는 자신만의 로직을 가질 수 있음

**추가 고려사항. 일괄처리 크기 (대역폭과 응답 지연 사이의 그 어딘가)**

서비스의 용도에 따라 조정이 필요!

- 일괄처리 메시지 양이 크면 응답 속도가 느려짐
- 일괄처리 메시지 양이 적으면 대역폭 손해

**소비자 측 작업 흐름**

아래 그림처럼 소비자는 오프셋을 활용하여 해당 위치에서부터 메시지를 가져옴

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%208.png)

메시지를 누가, 어떻게 처리할지 고려사항!

- From broker: 브로커가 소비자에게 보낼 것인가?
- From consumer: 소비자가 브로커로부터 가져갈 것인가?

**첫번째 방법. 푸시(push) 모델**

장점

- 브로커는 메시지를 받는 즉시 소비자에게 보냄 (낮은 지연)

단점

- 소비자가 메시지 처리하는 속도가 느릴 경우, 부하 발생
- 생산자의 데이터 전송 속도에 맞는 소비자 컴퓨팅 자원이 필요

<aside>
💡 소비자는 생산자의 성능과 비슷해야 함
(설계상으로는 디커플링이지만 운영상에서는 강한 커플링)

</aside>

**두번째 방법. 풀(pull) 모델**

장점

- 메시지 소비 속도를 소비자가 결정
    - 소비 속도가 느려지면?! 소비자를 늘려 해결하면 됨
- 일괄 처리에 적합 (브로커 버퍼에 쌓여있는 메시지를 한번에 가져올 수 있기 때문)

단점

- 컴퓨팅 자원 낭비 (메시지가 없어도 지속적으로 시도)

<aside>
💡 전반적으로 푸시 모델보다 유연한 모델
(단점은 long polling으로 해결 할 수 있음)

</aside>

결과적으로 풀 모델로 아래와 같은 시나리오가 가능

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%209.png)

1. 코디네이터가 소비자를 그룹에 참여시키고 파티션 할당
2. 소비자는 해당 파티션에서 오프셋 이후 메시지를 가져옴
3. 소비자는 메시지 처리 후, 오프셋 갱신

**소비자 재조정(consumer rebalancing)**

소비자가 어떤 파티션을 사용할지 다시 정하는 프로세스

- 소비자 추가, 삭제, 장애 또는 파티션이 조정되는 경우에 필요함

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2010.png)

위 그림을 이해하기 위해 필요한 부분이 코디네이터(coordinator)

- 소비자 재조정을 위해 소비자들과 통신하는 브로커 노드
- 소비자로부터 오는 heartbeat 메시지를 살피고 각 소비자의 파티션 내 오프셋 정보 관리

<aside>
💡 참고로 각 소비자는 특정 그룹에 속하며 리더 브로커(코디네이터)에서 소비자 그룹 중 리더 소비자를 선출하고, 선출된 리더는 파티션 배치 계획(partition dispatch plan)을 코디네이터에게 전달

- 리더를 제외한 그룹 내 소비자는 코디네이터로부터 파티션 배치 계획을 알게됨

![Consumer Group Coordination](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2011.png)

Consumer Group Coordination

</aside>

- 카프카 재조정 예시
    
    [Kafka Consumer Group Rebalance (1 of 2)](https://medium.com/lydtech-consulting/kafka-consumer-group-rebalance-1-of-2-7a3e00aa3bb4)
    

**상태 저장소 (state storage)**

- 소비자 파티션 배치 저장

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2012.png)

해당 저장소의 사용 패턴을 대략 추측해보면,

- 읽기와 쓰기가 빈번하게 발생하지만 양은 적음
- 데이터 갱신이 빈번하게 일어나지만 삭제는 거의 되지 않음
- 읽기와 쓰기 연산은 무작위
- 데이터 일관성이 중요

정리하자면 데이터 일관성 및 높은 읽기, 쓰기 속도 등으로 이에 적합한 기술은 키-값 모델을 사용하는 주키퍼(Apache ZooKeeper)와 같은 저장소가 필요함.

- 카프카는 작별을 고함(Kafka No Longer Requires ZooKeeper)
    
    [동물원을 탈출한 카프카 — Zookeeper-less Kafka](https://psm1782.medium.com/동물원을-탈출한-카프카-zookeeper-less-kafka-a71cba58d5d9)
    
    - 관리 복잡성만 높아졌었음
        - Kafka cluster와 ZooKeeper 함께 관리를 했어야만함
    - 외부 메타데이터 스토어로 카프카 자체의 확장성을 제한시켰음
        - 시간이 지남에 따라 날로 커지는 메타데이터의 크기로 인해 클러스터 내의 파티션 수에 제한이 있었음 (메타데이터를 불러올 때, 리소스를 많이 소비했음)
    
    즉, 카프카는 ZooKeeper 의존성으로 자체 확장성에 제약이 걸렸었기 때문에 외부 시스템 의존성을 떼어내는게 어떻게든 필요했었음
    

**메타데이터 저장소**

파티션 수, 메시지 보관 기간 등등과 같은 토픽 설정이나 속성 정보를 보관

사용 패턴은 자주 변경되지 않으며 양도 적지만 높은 일관성을 요구함

참고로, 해당 책은 이런 저장소는 다 ZooKeeper를 추천함

**복제**

하드웨어 장애는 흔한 일이기 때문에 가용성을 필요로하는 시스템에서는 전통적인 기법인 복제를 사용함

- 카프카 예시
    
    ![Kafka replication](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2013.png)
    
    Kafka replication
    

**사본 동기화**

이전 챕터의 연속되는 개념으로 데이터를 여러 노드에 어떻게 동기화 시킬지에 대한 부분

이때 등장하는 개념이 ISR(In-Sync Replicas, ISR)

- 성능과 영속성(durability) 사이의 타협점
(최대한 빠르게 처리할 것인지, 안전하게 처리할 것인지)

![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2014.png)

위 그림에서 합의 오프셋(committed offset)은 13

- 새로운 메시지(합의 오프셋 14, 15)가 들어왔지만, 모든 사본에 동기화가 끝난 것은 13까지
(사본 2, 3은 ISR 이며, 사본 4는 아직 ISR이지 않음)

**사본 동기화 - 수신 응답 설정(ACK=all)**

생산자는 모든 ISR이 메시지를 수신한 뒤에 ACK 응답 받음

- 느린 ISR 응답 및 메시지를 보내는 시간이 김
- 영속성 좋음

![ACK=all](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2015.png)

ACK=all

메시지는 최대한 보장하겠다는 전략

**사본 동기화 - 수신 응답 설정(ACK=1)**

리더만 메시지 받고 응답

- 간헐적인 영속성(메시지 보낸 직후 리더가 장애가 생기면 복구 불가능)

![ACK=1](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2016.png)

ACK=1

반반 무많이 전략(설마 리더가 장애가 생기겠어? 라는…)

**사본 동기화 - 수신 응답 설정(ACK=2)**

응답은 필요치 않음

- 최대치의 낮은 응답 지연

![ACK=0](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2017.png)

ACK=0

뒤가 없는 올인 전략

**규모 확장성**

기본 설계 외에 주요 시스템 컴포넌트 별로 확장성에 대한 고려가 필요

**생산자**

- 단순히하게 생산자를 추가하거나 삭제

**소비자**

- 소비자 그룹(consumer group) 및 재조정(rebalancing)

**브로커**

규모 확장성을 위해 장애 복구도 같이 고려가 필요

![브로커 노드 장애 복구](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2018.png)

브로커 노드 장애 복구

위의 그림을 간추려보면 기본 장애 복구 시나리오는 아래와 같이 2가지 순서이며 보관하는 데이터를 잃어버리지 않도록 하는 것이 매우 중요함

1. 브로커 컨트롤러에서 특정 브로커의 장애 인지후 파티션 분산
2. 새로 추가된 사본은 리더에 보관 메시지 복사
- 더 나은 결함 내성을 위한 생각 포인트
    - 얼마나 많은 사본에 메시지가 반영되어야 성공적인 합의(committed)라고 할 수 있을까?
    - 사본은 각각 다른 노드로 위치
    - 데이터 안전성, 리소스 유지 비용, 응답 지연 등을 고려한 사본 수와 사본 위치
        - 물리적인 위치는 국가간, 지역간, Rack간?
    
    ![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2019.png)
    

결함 내성 방식을 기반으로 규모 확장도 살펴보면 생각보다 간단하게 해결할 수 있음

- 한시적으로 설정된 사본 수보다 많은 사본을 허용
(추가된 사본에 복제하고 기존 것을 지우면 되기 때문)

**파티션**

몇몇 운영상의 이유로 파티션 수 조정이 필요하며, 소비자는 브로커로부터 통지를 받고 재조정

- 토픽의 규모를 늘릴 때
- 대역폭 조정
- 기타 등등

파티션은 추가, 삭제 2가지 경우를 살펴볼 수 있는데 현시점의 보관 메시지를 기준 진행

- 파티션 추가
    - 보관된 메시지(persistent messages)는 기존 파티션으로 유지 (데이터 이동X)
    - 파티션이 추가된 이후 오는 메시지를 보관
- 파티션 삭제
    - 삭제 될 파티션을 제외한 파티션으로 메시지 보관
    - 일정 시간 유지 후 삭제 (파티션의 데이터를 읽고 있는 소비자가 있을 수 있기에)
    - 소비자는 파티션이 실제 제거될 때, 재조정 작업

**메시지 전달 방식**

**최대 한 번(at-most once)**

- 메시지가 전달 과정에서 소실되더라도 다시 전달하지 않음
- 생산자
    - 메시지를 보내고 수신 응답 기다리지 않음
    - 실패해도 다시 시도하지 않음
- 소비자
    - 오프셋 갱신 이후 소비자가 장애로 죽으면 메시지가 다시 소비되지 않음
- 지표 모니터링 등 소량의 데이터 손실을 감수 할 수 있는 애플리케이션에 적합

**최소 한 번(at-least once)**

- 한 번 이상 전달될 수 있으나 메시지 소실은 발생하지 않음
- 생산자
    - 메시지가 브로커에게 전달되었음을 반드시 확인, 계속 재시도
- 소비자
    - 데이터를 성공적으로 처리한 뒤에만 오프셋 갱신
- 데이터 중복이 큰 문제가 아닌 애플리케이션 또는 중복을 직접 제거할 수 있는 애플리케이션

**정확히 한 번(exactly once)**

시스템의 성능 및 구현 복잡도 측면에서 큰 대가

**고급 기능**

**메시지 필터링**

첫번째 접근. 전용 토픽 사용

- 다른 시스템도 매번 전용 토픽을 만들도록 할 것인가?
- 같은 메시지를 여러 토픽에 저장하는 건 자원 낭비
- 오히려 생산자와 소비자 사이의 결합이 강해짐

두번째 접근. 소비자 담당

- 소비자가 모든 메시지를 받고 난 뒤에 처리
    - 불필요한 트래픽 발생으로 시스템 성능 저하

세번째 접근. 브로커 담당

- 브로커에서 메시지 필터링
    - 페이로드가 아닌 메타데이터 영역 (태그)

<aside>
💡 해당 책에서는 간단한 메타데이터에 태그를 삽입하여 필터링하는 것을 효과적이라고 판단하고 있음

</aside>

- 태그 기반 필터링
    
    ![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2020.png)
    
    [Message Filtering | RocketMQ](https://rocketmq.apache.org/docs/featureBehavior/07messagefilter/)
    

**메시지의 지연 전송 및 예약 전송**

토픽을 임시 저장소로 활용하여 시간이 되면 발송(또는 소비되고 있는 토픽으로)

- 토픽 활용 예시
    
    ![공통 알림 토픽 구성](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2021.png)
    
    공통 알림 토픽 구성
    
- 타이밍 기능 (timing function)
    
    [TimingWheel을 이용한 타이머 구현](https://d2.naver.com/helloworld/267396)
    
    - 타이머 간격을 상세하게 할 수록 슬롯(slot)의 개수가 많아짐
    - 슬롯의 개수는 메모리를 의미하므로 타이머 간격이 상세하면 메모리 사용량이 많아짐
    
    단순하게 사용한다면 Timing Wheels 구조를 그대로 사용해도 무방하지만, 그렇지 않다면 Hierarchical Timing Wheels 구조를 사용하는게 나을 수 있음
    
    [Apache Kafka, Purgatory, and Hierarchical Timing Wheels | Confluent](https://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels/)
    
    ![Untitled](/208/assets/img/study/books/system-design-interview-vol2/chapter04/Untitled%2022.png)
    
    - 낮은 단계의 Wheels는 가장 촘촘한 시간 간격을 처리하고, 상위 레벨로 올라갈 수록 시간 간격이 커지는 방식
        - 상위 단계의 휠은 on-demand로 생성하는 방식으로 메모리 사용이 보다 효율적임
    

### 추가적으로 다루어 볼만한 것

- 프로토콜
    - heartbeat 교환 등의 노드간 통신 정책
- 메시지 소비 재시도(retry consumption)
- 이력 데이터 아카이브(historical data archive)
    - 삭제된 메시지를 다시 처리하고 싶은 소비자가 있다면?
    - 내부 시스템에서 사용하지 않는 추가적인 저장소가 필요할 것인가?
- Archive storage
    
    [Amazon S3 Glacier 스토리지 클래스 | AWS](https://aws.amazon.com/ko/s3/storage-classes/glacier/)
    
    [Data Warehousing | AWS Solutions for Analytics | AWS Solutions Library](https://aws.amazon.com/ko/solutions/analytics/data-warehousing/)
    

위에 추가적으로 다루어 볼만한 것들이 나열되어 있지만, 인터뷰에서는 지금까지 언급된 것 중 반도 못하고 나올 것이 분명해보임

### 첨부 자료

[Kafka.pdf](4%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A1%E1%86%AB%20%E1%84%86%E1%85%A6%E1%84%89%E1%85%B5%E1%84%8C%E1%85%B5%20%E1%84%8F%E1%85%B2%20db29713ce5e84c86a2aa2e34e1bd7830/Kafka.pdf)