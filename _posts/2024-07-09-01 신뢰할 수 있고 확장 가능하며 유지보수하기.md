---
layout: post
title:  "01. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션"
date:   2024-07-09 00:00:00 +0900
categories: study books data-intensive-applications
---
발표자: 한재민
발표일: 2024년 7월 9일

오늘날의 많은 애플리케이션은 계산 중심(compute-intensive) 보다는 **데이터 중심(data-intensive)**

즉, 애플리케이션의 성능을 좌지우지 하는 것은 CPU 보다는 데이터 양, 복잡도, 변화 속도라는 것.

<aside>
💡 일반적으로 데이터 관련 애플리케이션은 생각없이 사용할 수 있음

왜?! 데이터 시스템이 잘 추상화되어 있기 때문에!

- 예시
    - 데이터베이스: 데이터 저장
    - 캐시: 읽기 속도 향상
    - 검색 색인: 키워드 검색 혹은 다양한 방법으로 필터링
    - 스트림 처리: 비동기 메시지 처리
    - 일괄 처리: 주기적으로 대량의 데이터 분석
</aside>

무작정 데이터 시스템을 사용하기에는 현실은 녹록치 않음.
(애플리케이션마다 요구사항이 다를 뿐더러 데이터 시스템 마다 특성이 다르기 때문)

데이터 시스템의 원칙(principle)과 실용성(practicality) 등을 고려하여 판단해야 함.

# 데이터 시스템에 대한 생각

먼저 이 책에서 여러 도구들을 하나의 데이터 시스템으로 포괄적이게 묶는 이유는

1. 몇몇 도구들은 usecase에 의해 등장했기 때문에 명확하게 구분하기는 어려워지고, 
2. 아래 예시처럼 여러 개의 도구들을 활용하여 애플리케이션을 구성하기 때문

**개발자가 애플리케이션 개발 뿐아니라 데이터 시스템 설계까지 해야함**

![다양한 구성 요소를 결합한 데이터 시스템 아키텍처의 예](/208/assets/img/study/books/data-intensive-applications/chapter01/Untitled.png)

다양한 구성 요소를 결합한 데이터 시스템 아키텍처의 예

설계 관점으로 보면, 아래와 같은 고려할 요소들이 많아짐

- 데이터를 정확하게 완전하게 유지하려면?
- 일관된 성능을 유지하기 위해서는?
- 부하가 많아진다면?
- 등등

이를 3가지 관점으로 추려보면 신뢰성(Reliability), 확장성(Scalability), 유지보수성(Maintainability)

# 신뢰성

결함이나 오류가 발생하더라도 시스템은 지속적으로 올바르게 동작해야 한다는 것

<aside>
💡 잘못될 수 있는 일은 결함(fault)
결함을 예측하고 대처할 수 있는 시스템은 내결함성(fault-tolerant) 또는 탄력성(resilient)

</aside>

여기서 모든 종류의 결함에 대해 대처할 수 있는 것은 불가능함. **특정 유형에 대해서만 대처한다는 것**.

참고로 결함과 장애(failure)에 대해 헷갈릴 수 있는데, 

- 결함: 사양에서 벗어난 시스템의 한 구성 요소
- 장애: 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우

결함으로 인해 시스템이 장애가 발생하지 않도록 내결함성 구조를 갖추어야 함

> 많은 버그들은 미흡한 오류처리로 인해 발생하기 때문에, 넷플릭스의 ‘카오스몽키’와 같은 도구는 고의적으로 결함을 유도하여 내결함성을 테스트
> 

[https://github.com/Netflix/chaosmonkey](https://github.com/Netflix/chaosmonkey)

### 하드웨어 결함

하드디스크 고장, 램 결함, 네트워크 케이블 결함 등등은 늘상 일어나는 하드웨어 결함임.
(10,000개의 디스크로 구성된 저장 클러스터는 평균적으로 하루에 한 개의 죽을 수 있다고 함)

가능한 대응 방식으로는 일반적으로 중복(redundancy)

- 디스크는 RAID
- 서버는 이중 전원 디바이스, 핫 스왑 CPU
- 등등

과거에는 서비스의 중단시간(downtime)이 치명적이지 않았기 때문에 필수적인 소수의 애플리케이션에서만 사용했지만, 지금은 **데이터 양이 늘어나면서 당연한 것으로 인지되고 있음**

- 클라우드 플랫폼 예시 - AWS
    
    가상 장비 인스턴스가 별도의 경고 없이 사용할 수 없게 되는 상황이 상당히 일반적으로 많은 수의 장비를 사용하면서, 이에 비례하여 하드웨어 결함율도 증가하기 때문임
    
    [AWS: the good, the bad and the ugly](https://www.talisman.org/~erlkonig/misc/aws-the-good-the-bad+the-ugly/)
    

최근에는 **소프트웨어 내결함성 기술을 사용하거나 하드웨어 중복성을 추가**하여 전체 장비의 손실을 견딜 수 있는 시스템으로 옮겨가고 있는 추세

### 소프트웨어 오류

시스템 내 체계적 오류(systematic error)

- 잘못된 특정 입력이 있을 때 모든 애플리케이션 서버 인스턴스가 죽는 버그
- 공유 자원을 과도하게 사용하는 일부 프로세스
- 시스템 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스
- 한 구성 요소의 작은 결함이 다른 구성 요소릐 결함을 야기하는 연쇄 장애

위의 버그들은 특정 상황에 의해 발생하기 전까지 오랫동안 나타나지 않기 때문에 예상하기가 보다 어려움

**소프트웨어의 체계적 오류 문제는 신속한 해결책이 없고, 모니터링과 같은 분석 도구들을 통해 문제 해결을 할 수 밖에 없음 혹은 예방 알림**

### 인적 오류

사람의 실수에 의한 오류

- 오류의 가능서을 최소화하는 방향으로 시스템 설계 (너무 제한적이면 오히려 문제)
- 실수로 장애가 발생할 수 있는 부분을 분리 (ex: 샌드박스 제공)
- 단위 테스트부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트
- 장애 발생의 영향을 최소화하기 위해 인적 오류를 빠르고 쉽게 복구할 수 있게 하라
- 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책 마

# 확장성

현재 시스템이 미래에도 안정적으로 동작하는 보장은 없음. 부하 증가 때문에!

### 부하 기술하기

부하는 부하 매개변수(load parameter)라 부르는 몇 개의 숫자로 나타낼 수 있음

- 웹 서버의 초당 요청 수
- 데이터베이스의 읽기 대 쓰기 비율
- 대화방의 동시 활성 사용자
- 캐시 적중률
- 등등

**부하 매개변수의 선택은 시스템의 설계에 따라 달라지며, 평균적인 경우가 중요할 수 있고 소수의 극단적인 경우가 병목 현상의 원인이 될 수 있음**

- 트위터 예시
    
    ![초당 12,000건의 쓰기 + 팔로우 fan-out](/208/assets/img/study/books/data-intensive-applications/chapter01/Untitled%201.png)
    
    초당 12,000건의 쓰기 + 팔로우 fan-out
    
    **접근 1 - 관계형 데이터베이스 쿼리**
    
    쿼리 부하
    
    **접근 2 - 홈 타임라인 캐시 유지**
    
    캐시를 통한 읽기 속도 향상
    
    반면, 트윗 작성에 따른 많은 부가 작업
    
    **접근 3 - 하이브리드**
    
    팔로워 수가 많은 소수 사용자는 접근 1의 방식, 나머지는 접근 2의 방식
    

### 성능 기술하기

시스템 부하 관련해서 서술할 때 두가지 방법으로 살펴볼 수 있음

- 부하 매개변수를 증가시키고 시스템 자원은 변경하지 않고 유지하면 시스템 성능은 어떻게 영향을 받을까?
- 부하 매개변수를 증가시켰을 때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 확보해야할까?
- 성능 기술 예시
    - 하둡과 같은 시스템이라면 처리량(초당 처리할 수 있는 레코드 수)
    - 온라인 시스템이라면 응답 시간

> **응답 시간**은 클라이언트 관점의 시간으로 요청 처리하는 시간 외에도 네트워크 지연과 큐 지연 등을 포함, **지연시간**은 요청이 처리되길 기다리는 시간으로 휴지(latent) 상태 시간
> 

응답 시간은 단일 숫자가 아닌 측정 가능 값의 분포로 간주해야 함
(오래 걸리는 특이 값(outlier)이 있을 수 있는데, 관련하여 여러 원인이 있을 수 있기 때문임)

- 추가 지연의 원인
    1. 백그라운드 프로세스의 컨텍스트 스위칭
    2. 네트워크 패킷 손실과 TCP 재전송
    3. 가비지 컬렉션 휴지(garbage collection pause)
    4. 디스크에서 읽기를 강제하는 페이지 폴드(page fault)
    5. 서버 랙의 기계적인 진동
    6. 등등

서비스의 응답 시간은 일반적으로 산술 평균으로 판단할 수 있겠지만 좋은 지표가 아님
(평균의 함정)

**평균보다는 백분위(percentile)를 사용하는 편이 더 나음**

> 응답 시간 목록을 빠른 시간부터 나열한 중간 지점이 중앙값(median)으로 50분위(p50)으로 축약할 수 있다. 이를 바탕으로 특이 값이 얼마나 좋지 않은지 알아보려면 상위 백분위인 95분위(p95), 99분위(p99), 99.9분위(p999)를 사용하는 것이 일반적이다.
> 

상위 백분위는 꼬리 지연 시간(tail latency)로 표현하는데 서비스의 UX에 직접 영향을 주며, 아마존은 내부 서비스의 응답 시간 요구사항을 99.9분위로 기술함
(응답 시간이 가장 느린 요청을 하는 고객은 많은 구매를 하기 때문임)

<aside>
💡 아마존은 응답 시간이 100밀리초 증가하면 판매량이 1% 줄어들고 1초가 느려지면 고객 만족도 지표는 16% 줄어든다고 함

</aside>

참고로 백분위는 서비스 수준 목표(service level objective, SLO)와 서비스 수준 협약서(service level agreement, SLA)에서 자주 사용함

### 부하 대응 접근 방식

보통은 용량 확장(scaling up)과 규모 확장(scaling out)

<aside>
💡 현실적으로 좋은 아키텍처는 실용적인 접근 방식의 조합이 요구되며, 실제로 적절한 사양의 장비 몇 대가 다량의 낮은 사양 가상 장비보다 훨씬 간단하고 저렴함

</aside>

일부 시스템은 탄력적(elastic)으로 부하 증가를 감지하면 컴퓨팅 자원을 자동으로 추가 할 수 있음

다만, **탄력적이 시스템은 예측할 수 없을 때 유용하지만 수동 확장이 더 간단하고 운영상 예상치 못한일이 더 적음**

대개 대규모로 동작하는 시스템의 아키텍처는 애플리케이션에 특화되어 있기 때문에 범용적인 아키텍처는 없음

- 아키텍처를 결정하는 요소
    - 읽기 양
    - 쓰기 양
    - 저장할 데이터 양
    - 데이터 복잡도
    - 응답 시간 요구사항
    - 접근 패턴
    - 등등

<aside>
💡 동일한 데이터 처리를 한다하더라도 애플리케이션에 따라 크게 달라질 수 있으며, 주요 동작에 따른 올바른 부하 매개변수를 따져보는 것이 매우 중요
(잘못된 부하 매개변수 가정은 역효과만 낳을 수 있음)

</aside>

# 유지보수성

소프트웨어 비용의 대부분은 초기 개발보다는 지속해서 이어지는 유지보수에 들어감

레거시 시스템의 고통을 최소화하기 위한 시스템 설계 원칙은 세가지. 운용성, 단순성, 발전성

### 운용성

운영팀이 시스템을 원활하게 운영할 수 있도록 쉽게 만들어야 함
즉, 좋은 운영성이란 동일하게 반복되는 태스크를 쉽게 수행하게 만들어야한다는 것

### 단순성

복잡도를 최대한 제거하여 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어야 함

### 발전성

엔지니어가 이후에 시스템을 쉽게 변경할 수있도록 해야