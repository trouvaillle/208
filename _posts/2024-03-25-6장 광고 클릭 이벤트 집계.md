---
layout: post
title:  "6장.  광고 클릭 이벤트 집계"
date:   2024-03-25 00:00:00 +0900
categories: study books system-design-interview-vol2
announced_on: 2024년 3월 25일
author: 홍성민
---
디지털 광고의 핵심 프로세스는 RTB(Real-Time Bidding), 즉 실시간 경매라 부른다. 광고가 나갈 지면을 거래한다. 광고 클릭 이벤트 집계는 온라인 광고가 얼마나 효율적이었는지 측정하는데 결정적인 역할을 하며 광고주가 얼마나 많은 돈을 지불할지에 영향을 끼친다. 

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled.png" | relative_url }})

# 1. 문제 이해 및 설계 범위 확정

### **기능 요구사항**

- 지난 M분 동안의 ad_id 클릭 수 집계
- 매분 가장 많이 클릭된 상위 100개 광고 아이디를 반환
- 다양한 속성에 따른 집계 필터링을 지원
- 데이터의 양은 페이스북이나 구글 규모

### **비기능 요구사항**

- 집계 결과 정확성은 데이터가 RTB 및 광고 과금에 사용되므로 중요
- 지연되거나 중복된 이벤트를 적절히 처리할 수 있어야 함
- 견고성(reliability): 부분적인 장애는 감내할 수 있어야 함
- 지연 시간 요구사항: 전체 처리 시간은 최대 수 분을 넘지 않아야 함

# 2. 개략적 설계안 만들기

### **질의 API 설계**

1. 지난 M분간 각 ad_id에 발생한 클릭 수 집계

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/cb68d3d3-4758-4623-bc8f-558cff0b4cdc.png" | relative_url }})

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/63596c0c-49cc-4988-8567-beb8e8d08a5b.png" | relative_url }})

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/b098da34-969a-4fc7-9c41-f12c3e7442a8.png" | relative_url }})

1. 지난 M분간 가장 많은 클릭이 발생한 상위 N개 ad_id 목록

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%201.png" | relative_url }})

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%202.png" | relative_url }})

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%203.png" | relative_url }})

### **데이터 모델**

이 시스템이 다루는 데이터는 두 종류로 나눌 수 있다. 즉 원시 데이터(raw data)와 집계 결과 데이터(aggregated)다 .

**원시데이터**

이런 데이터가 여러 애플리케이션 서버에 산재해 있는 상황이다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%204.png" | relative_url }})

**집계 결과 데이터**

광고 클릭 이벤트가 매분 집계된다고 가정했을 때의 집계 결과다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%205.png" | relative_url }})

광고 필터링을 지원하기 위해 filter_id를 추가한다. 

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%206.png" | relative_url }})

필터를 정의한 테이블은 아래와 같다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%207.png" | relative_url }})

지난 M분 동안 가장 많이 클릭된 상위 N개의 광고를 반환하는 질의를 지원하기 위해서는 다음 구조를 사용한다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%208.png" | relative_url }})

문제 발생 시 디버깅에 활용할 수 있도록 원시 데이터를 보관하고 질의 성능을 높이기 위해 집계 결과 데이터도 사용한다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%209.png" | relative_url }})

### 데이터베이스 선택

이 광고 시스템은 클릭 이벤트를 받는 것이 중심인 시스템이다. 즉 쓰기 중심 시스템이다. 쓰기 및 시간 범위 질의에 최적화된 카산드라나 InfluxDB를 사용하는 것이 좀 더 바람직하다.

![[https://www.isaacbigdata.com/performance-cassandra](https://www.isaacbigdata.com/performance-cassandra/)]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2010.png" | relative_url }})

[https://www.isaacbigdata.com/performance-cassandra](https://www.isaacbigdata.com/performance-cassandra/)

### 개략적 설계안

클릭 이벤트 수가 소비자의 처리 용량을 훨씬 넘어서는 경우 소비자는 메모리 부족 오류 등의 문제가 생길 수 있다. 카프카 같은 메시지 큐를 도입하여 생산자와 소비자의 결합을 끊는다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2011.png" | relative_url }})

집계 결과를 데이터베이스에 바로 기록하지 않는 이유는 정확하게 한 번(exactly once) 데이터를 처리하기 위해서이다. 그러기 위해서는 (atomic commit, 즉 원자적 커밋) 카프카 같은 시스템을 두 번째 메시지 큐로 도입해야한다.

### 집계 서비스

광고 클릭 이벤트를 집계하는 좋은 방안 하나는 맵리듀스(MapReduce) 프레임워크를 사용하는 것이다.

대표적으로 **Hadoop MapReduce, Apache Spark, Cloud Dataflow 등이 있다.**

**맵 노드**

데이터 출처에서 읽은 데이터를 필터링하고 변환하는 역할을 담당한다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2012.png" | relative_url }})

**집계 노드**

집계 노드는 ad_id별 광고 클릭 이벤트 수를 매 분 메모리에서 집계한다.  리듀스 프로세스의 일부다.

**리듀스 노드**

집계 노드가 산출한 결과를 최종 결과로 축약한다. 리듀스 노드는 결과를 모아 최종적으로 3개의 광고만 남긴다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2013.png" | relative_url }})

**주요 사용 사례**

사례 1:지난 M분간 ad_id에 발생한 클릭 이벤트 수 집계

맵 노드는 시스템에 입력되는 이벤트를 ad_id % 3을 기준으로 분배하며, 이렇게 분배한 결과는 각 집계 노드가 집계한다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/af6046b9-8777-4eac-af67-ba66974a42d1.png" | relative_url }})

사례 2: 지난 M분간 가장 많은 클릭이 발생한 상위 N개의 ad_id 집계

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2014.png" | relative_url }})

사례 3: 데이터 필터링

“미국 내 광고 ad001에 대해 집계된 클릭 수만 표시” 같은 데이터 필터링을 지원하려면 필터링 기준을 사전에 정의한 다음에 해당 기준에 따라 집계하면 된다. 예를 들어 기준을 ad001과 ad002에 적용하면 아래 표와 같다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2015.png" | relative_url }})

이런 기법은 스타 스키마라고 부른다 데이터 웨어하우스에서 널리 쓰이는 기법으로 필터링에 사용되는 필드는 차원이라 부른다. 이 접근 법은 미리 계산해 두는 방식으로 데이터에 빠르게 접근할 수 있다. 하지만 많은 버킷 레코드가 생성된다는 한계가 있다. 필터링 기준이 많을 경우 더욱 그렇다.

# 3. 상세 설계

**데이터 재계산**

집계 서비스에 버그가 있었다고 가정하자. 버그 발생 시점부터 원시 데이터를 다시 읽어 집계 데이터를 재계산하고 고쳐야 할 것이다. 

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2016.png" | relative_url }})

1. 재계산 서비스는 원시 데이터 저장소에서 데이터를 검색한다. 일괄 처리 프로세스를 따른다.
2. 추출된 데이터는 전용 집계 서비스로 전송된다. 전용 집계 서비스를 두는 것은 실시간 데이터 처리 과정이 과거 데이터 재처리 프로세스와 간섭하는 일을 막기 위해서다.
3. 집계 결과는 두 번째 메시지 큐로 전송되어 집계 결과 데이터베이스에 반영된다.

**시간**

집계를 하려면 타임스탬프가 필요하다. 

- 이벤트 시각: 광고 클릭이 발생한 시각
- 처리 시각: 집계 서버가 클릭 이벤트를 처리한 시스템 시각

네트워크 지연이나 비동기적 처리 환경 때문에 이벤트가 발생한 시각과 처리 시각 사이의 격차는 커질 수 있다. 

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2017.png" | relative_url }})

데이터 정확도가 중요하므로 여기서는 이벤트 발생 시각을 사용할 것을 추천한다. 이 경우 시스템에 늦게 도착한 이벤트를 올바르게 처리하기 위해 ‘워터마크’ 라는 기술을 일반적으로 사용한다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2018.png" | relative_url }})

집계 윈도의 확장으로 본다. 이렇게 하면 집계 결과의 정확도를 높일 수 있다. 가령 15초 워터마크를 윈도마다 붙이면 윈도1은 이벤트2를 윈도4는 이벤트5를 집계할 수 있게 된다. 워터마크의 크기는 비즈니스 요구사항에 따라 달리 잡는다. 워터마크 구간이 길면 늦게 도착하는 이벤트도 포착할 수 있지만 시스템의 이벤트 처리 시간은 늘어난다. 워터마크가 짧으면 데이터 정확도는 떨어지지만 시스템의 응답 지연은 낮아진다.

 워터마크 기법으로 아주 늦게 시스템에 집계된 이벤트는 처리할 수 없다. 이를 처리하기 위한 설계는 ROI가 떨어진다. 게다가 조정을 통해 하루치 데이터 처리를 마감할 때 수정할 수도 있다.

**집계 윈도**

1. 덤블링윈도

매 분 발생한 클릭 이벤트를 집계하기에 적합

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2019.png" | relative_url }})

1. 슬라이딩 윈도

지난 M분간 가장 많이 클릭된 상위 N개 광고를 알아내기에 적합

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2020.png" | relative_url }})

**전달 보장**

집계 결과는 과금 등에 활용될 수 있기 때문에 데이터의 정확성과 무결성이 아주 중요하다. 

- 이벤트의 중복 처리를 피할 수 있어야한다.
- 모든 이벤트의 처리를 보장해야한다.

카프카가 지원하는 전달 방식

- 최대 한 번(at-most once)
- 최소 한 번(at-least once)
- 정확히 한 번(exaclty once)

과금과 관련있으니 ‘정확히 한 번’ 전달 방식을 사용한다.

다음과 같은 경우 서버 장애로 중복 데이터가 생길 수 있다. 

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2021.png" | relative_url }})

6단계를 실행하지 못하면 100에서 110까지의 이벤트는 이미 다운스트림에 전송되었으나 새 오프셋은 업스팀 카프카에 반영되지 않았다. 따라서 복구된 집계 서비스 노드는 오프셋 100부터 이벤트를 다시 소비하려 할 것이고 그 결과로 데이터 중복이 발생한다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2022.png" | relative_url }})

이벤트를 정확하게 한 번만 처리하고 싶다면 4단계부터 6단계까지의 작업을 하나의 분산 트랜잭션에 넣어야 한다. 분산 트랜잭션은 여러 노드에서 작동하는 트랜잭션으로, 그 안에서 실행하는 작업 가운데 하나라도 실패하면 모든 작업의 상태를 실행 전으로 되돌리게 된다. 

## 시스템 규모 확장

**브로커**

- 해시 키
    - ad_id로 잡으면 같은 ad_id를 갖는 이벤트를 전부 같은 파티션에서 구독할 수 있다.
- 파티션의 수
    - 파티션의 수가 변하면 같은 ad_id를 갖는 이벤트가 다른 파티션에 기록되는 일이 생길 수 있으니 사전에 충분한 파티션을 확보하는게 좋다.
- 토픽의 물리적 샤딩
    - 지역에 따라 여러 토픽을 둘 수도 있고 사업유형에 따라 둘 수도 있을 것이다.
    - 장점: 시스템 처리 대역폭을 높일 수 있다. 단일 토픽에 대한 소비자의 수가 줄면 소비자 그룹의 재조정 시간도 단축된다.
    - 단점: 복잡성이 증가하고 유지 관리 비용이 늘어난다.

**핫스팟 문제**

이벤트 파티션을 ad_id로 나누기 때문에 특정 서비스 노드에 과부하 문제가 발생할 수 있다. (특정 광고만 클릭율이 높은 경우)

1. 한 노드가 감당할 수 있는 양을 초과하는 많은 이벤트가 도착한 경우 자원 관리자에게 추가 자원을 요청
2. 추가 자원을 할당
3. 이벤트를 100개씩 3개 그룹으로 분할
4. 집계가 끝나 축약된 결과는 다시 원래 집계 서비스 노드에 기록

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2023.png" | relative_url }})

**결함 내성**

집계는 메모리에서 이루어지므로 집계 노드에 장애가 생기면 집계 결과도 손실된다. 하지만 업스트림 카프카 브로커에서 이벤트를 다시 받아오면 그 숫자를 다시 만들 수 있다.

업스트림 오프셋 같은 ‘시스템 상태’를 스냅숏으로 저장하고 마지막으로 저장된 상태부터 복구해 나가는 것이 바람직하다. 지난 M분간 가장 많이 클릭된 광고 N개 같은 데이터도 시스템 상태의 일부로 저장해야 한다.

스냅숏에 어떤 데이터가 보관보여주는 예시는 다음과 같다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2024.png" | relative_url }})

어떤 집계 서비스 노드 하나에 장애가 발생하면 해당 노드를 새 것으로 대체한 다음 마지막 스냅숏에서 데이터를 복구하면 된다. 스냅숏을 찍은 이후 도착한 이벤트는 새 집계 서비스 노드가 카프카에서 읽어 처리할 것이다.

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2025.png" | relative_url }})

**조정**

조정은 다양한 데이터를 비교하여 데이터 무결성을 보증하는 기법을 일컫는다. 매일 각 파티션에 기록된 클릭 이벤트를 이벤트 발생 시각에 따라 정렬한 결과를 일괄 처리하여 만들어 낸 다음 실시간 집계 결과와 비교해서 문제 여부를 찾을 수 있다. 

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2026.png" | relative_url }})

**-대안적 설계안**

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2027.png" | relative_url }})

광고 클릭 데이터를 하이브에 저장하고 빠른 질의는 일랙스틱서치 계층을 사용(하이브에 저장한 데이터를 분석돌리고 그 결과를 일라스틱서치에 저장해서 사용한다는 의미). 집계는 클릭하우스나 드루이드 같은 OLAP 데이터베이스를 통해 처리할 수 있다.

- 하이브(Hive)
    - Hadoop 상에서 대용량 데이터를 저장, 쿼리, 분석하기 위한 데이터 웨어하우스 인프라로, SQL과 유사한 HQL을 사용해 빅 데이터 분석을 손쉽게 할 수 있습니다.
    - 주로 배치 처리에 적합하며, 복잡한 데이터셋에 대한 분석과 보고서 생성에 사용됩니다.
    

하이브 원본데이터 샘플

- **`click_id`**: 클릭 이벤트의 고유 식별자
- **`user_id`**: 사용자의 고유 식별자
- **`ad_id`**: 클릭된 광고의 고유 식별자
- **`click_timestamp`**: 클릭 시간 (예: 'YYYY-MM-DD HH:MM:SS' 포맷)
- **`campaign_id`**: 광고 캠페인의 고유 식별자
- **`cost`**: 클릭 비용

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2028.png" | relative_url }})

```sql
SELECT
  DATE(click_timestamp) as click_date,
  campaign_id,
  COUNT(*) as total_clicks,
  SUM(cost) as total_cost
FROM
  ad_clicks
WHERE
  campaign_id = 301
GROUP BY
  DATE(click_timestamp),
  campaign_id
ORDER BY
  click_date;
```

하이브 분석 결과

![Untitled]({{ "/assets/img/study/books/system-design-interview-vol2/chapter06/Untitled%2029.png" | relative_url }})

- OLAP 데이터베이스: 집계 데이터를 여러 차원(지역, 날짜 등)으로 저장하고 빠르게 조회하는데 특화된 DB
    - ClickHouse:  고성능의 열 기반 관계형 데이터베이스 관리 시스템(RDBMS)입니다.
    - Druid:  분산형 OLAP 데이터베이스입니다.